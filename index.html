<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Tropical Geometry NN</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/sky.css">
		<link rel="stylesheet" href="css/custom.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					  <h2>
							<b>
								Tropical Geometry of Deep Neural Networks
							</b>
						</h2>
						Meg Walters - Data Scientist, Allstate <br>
						Venu Tammali - Data Scientist, Arity
						<br><br><br>
						<img src="arity.png", width="250", height="150">
						<img src="allstate.jpeg", width="250", height="150">
						<img src='D3_Logo2.jpg', width="250", height="150">
				</section>

				<section>
					<h3> <b>MOTIVATION </b></h3>
					<p style="font-size:20px">
					Deep neural networks have been very successful in a variety of
					applications across many different areas of computer vision, speech
					recognition and NLP.
					</br></br>
					Nevertheless, theoretical understanding of their efficacy remains incomplete.
					<br></br>
					There have been several attempts to analyze deep neural networks
					from different perspectives.
					<br></br><b>
					Notably, earlier studies have suggested
					that a deep architecture (More hidden layers) could use parameters
					more efficiently and requires exponentially fewer parameters to express
					certain families of functions than a shallow architecture
					(Less hidden layers but more neurons per layer)
					</b>
					</p>
				</section>

				<section>
					<h3><b>THINGS WE WILL LEARN</b></h3>
					<ul>
						<li><p style="font-size:30px">
							Connections between feedforward NN's with ReLU and tropical geometry
						</p>
						</li>
						<li><p style="font-size:30px">
							Family of feedforward NN's is equivalent to family of tropical rational maps
						</p>
						</li>
						<li><p style="font-size:30px">
							Zonotopes are the building blocks of deep NN's
						</p>
						</li>
					</ul>
				</section>

				<section>
					<h3><b>THINGS WE WILL LEARN CONTD.</b></h3>
					<ul>
						<li><p style="font-size:30px">
							Relate decision boundaries of feedforward NN's to tropical hypersurfaces
						</p>
						</li>
						<li><p style="font-size:30px">
							Introducing new measure of complexity of NN's - Geometric Complexity
						</p>
						</li>
						<li><p style="font-size:30px">
							Deeper network is exponentially more expressive than a shallow network
						</p>
						</li>
					</ul>
				</section>
				<section>
					<h3><b>THINGS WE WILL LEARN CONTD.</b></h3>

					<p style="font-size:20px">
					If we view a neural network as a function
					</p>

					<p style="font-size:30px">
					$\mathcal{v} : \mathbb{R}^d \rightarrow \mathbb{R}^p,
					x = (x_1, \dots, x_d) \rightarrow (\mathcal{v}_1(x),\dots,\mathcal{v}_p(x))$
					</p>
					<p style="font-size:20px">
					Then  each 	$\mathcal{v}$ is a tropical rational map, i.e., each 	$\mathcal{v}_i$  is a
					tropical rational function.
					</p>
					<p style="font-size:20px">
					In fact we will show the following three are equivalent
					</p>
					<ul>
						<li><p style="font-size:20px">
						Tropical rational functions
						</p>
						</li>
						<li><p style="font-size:20px">
						A feedforward NN with ReLU activation and one neuron in output layer
						</p>
						</li>
						<li><p style="font-size:20px">
						Continuous piecewise linear functions with integer coefficients
						</p>
						</li>
					</ul>
				</section>

				<section>
					<h3><b>PART - 1 - CONTENTS</b></h3>
          <br>
					<ul>
						<li>Neural network intro and notation</li>
						<li>Intro to tropical geometry and algebra</li>
					</ul>
				</section>


				<section>
						<h3><b>PART - 2 - CONTENTS</b></h3>
						<br>
						<ul>
						 <li>Tropical algebra of neural networks</li>
						 <li>Tropical geometry of neural networks</li>
					  </ul>
				</section>


				<section>
					<h3> <b>Intro to neural networks and notation</b></h3>
				</section>

				<section> <h3> Deep and Shallow Neural Network Architecture </h3>
					<img src="nn.png", {:height="75%" width="75%"}, style="border:0">
				</section>

				<section>
					<h3> Neuron </h3>
					<p style="font-size:20px">
						Neurons are the basic unit of a neural network. In nature, neurons have a number of dendrites (inputs),
						a cell nucleus (processor) and an axon (output). When the neuron activates, it accumulates all its
						incoming inputs, and if it goes over a certain threshold it fires a signal.
					</p>
					<img src="neuron.png", {:height="50%" width="50%"}, style="border:0">
					<p style="font-size:30px">
					 Where $S_j = \sum_{i=0}^{n} W_{ij} \cdot l_i$, and
					<br></br>
					$Y_j = f_j(S_j)$, where $f_j$ is an activation function.
				  </p>
				</section>

				<section> <h3> Activation function </h3>
					<p style="font-size:28px">
					 Activation functions set the output behavior of each neuron in a network.
					 This output then feeds into the next layer of neurons.
					 <br><br>
				  </p>
				  <h4> <b> Example activation functions </b> </h4>
					<table style='font-size:24px'>
						<tr>
							<th align="center"> Sigmoid </th>
							<th align="center"> Hyperbolic Tangent </th>
							<th align="center"> ReLU </th>
						<tr>
							<td> <img src="sigmoid.png", style="border:0"></td>
							<td><img src="tanh.png", style="border:0"</td>
							<td> <img src="relu.png", style="border:0"></td>
						</tr>
					</table>
				</section>

				<section>
 					<h3> Decision Boundary </h3>
 					<p style="font-size:20px">
 					Decision Boundary of a model illustrates the model's ability to seperate positive appending
 					negative dataset. Figures below illustrate why Neural Networks are so much more powerful.
           </p>
 					<img src="DatasetforDB.png", {:height="30%" width="30%"}, style="border:0">
 					<img src="LogisticRegressionDB.png", {:height="30%" width="30%"}, style="border:0">
 					<img src="3LayerNNDB.png", {:height="30%" width="30%"}, style="border:0">
 					<p style="font-size:20px">
 				  The above plots show that neural networks can learn more complex non-linear decision boundaries
 				 	</p>
 				</section>

				<section>
	 				<h3><b> Intro to tropical algebra and geometry </b></h3>
	 				  <image src="tropical.png">
	 			</section>

	 			<section><h3>Tropical Algebra</h3>
	 				<table style="width:20%">
	   				<tr>
	     				<th>Operation</th>
	     				<th>Notation</th>
	     				<th>Definition</th>
	   				</tr>
	 					<tr>
	 						<td>Addition</td>
	 						<td>$x\oplus y$</td>
	 						<td>$\max \{x, y\}$</td>
	 					</tr>
	 					<tr>
	 						<td>Multiplication</td>
	 						<td>$x\odot y$</td>
	 						<td>$x+y$</td>
	 					</tr>
	 					<tr>
	 						<td>Division</td>
	 						<td>$x\oslash y$</td>
	 						<td>$x-y$</td>
	 					</tr>
	 					<tr>
	 						<td>
	 							Power
	 							<small style="vertical-align:bottom">
	 								($a\in\mathbb{N}$)
	 							</small>
	 						</td>
	 						<td>$x^{\odot a}$</td>
	 						<td>$ax$</td>
	 				</table><br><br>
	 				<small>
	 					<p style="text-align:left">
	 						<i> There is no additive inverse, so this is a <b> semifield </b></i>
	 					</p>
	 				</small>
	 			</section>

	 			<section>
	 				<h3> Examples </h3>
	 				<br>
	 				$
	 				\begin{align}
	 				5\oplus 2 = 5 \\
	 				5\odot 2 = 7 \\
	 				5 \oslash 2 = 3\\
	 				5^{\odot 2} = 10 \\
	 				\end{align} $
	 				$$5\odot 2 \oplus 3 \odot 10 = 7\oplus 13 = 13$$

	 				<br><br>
	 				<small>
	 					<i> *Note the usual order of operations applies </i>
	 				</small>
	 			</section>

	 			<section>
	 				<h3> Tropical Monomials </h3>
	 				An expression of the form <br><br>
	 				<b>$$c\odot x_1^{a_1}\odot x_2^{a_2}\odot\dots\odot x_d^{a_d}$$</b>
	 				<br>
	 				$$=(c + a_1x_1 + a_2x_2 + \dots + a_dx_d)$$
	 				<br>
	 				where $c\in \mathbb{R} \cup \{-\infty\}$, $x\in\mathbb{R}$, $a_i\in\mathbb{N}$ <br><br>
	 				<div class="fragment">
	 					<h3> Example </h3>
	 					<p>
	 					$4\odot x_1^2\odot x_2^3 = $
	 						<scan class = "fragment">
	 							$4 + 2x_1 + 3x_2 $
	 						</scan>
	 					</p>
	 				</div>
	 			</section>

	 			<section>
	 				<h3> Tropical Polynomials </h3>
	 				An expression of the form <br><br>
	 				<b>
	 					$$f(x) = c_1x^{\alpha_1} \oplus c_2x^{\alpha_2}\oplus \dots\oplus c_rx^{\alpha_r}$$
	 				</b>
	 				<br>
	 				$$=\max(c_1 + \alpha_1x, c_2+\alpha_2x, \dots, c_r + \alpha_rx)$$
	 				<br>
	 				where $\alpha_i \in \mathbb{N}^d$, $x\in\mathbb{R}^d$ and $c_i\in \mathbb{R} \cup \{-\infty\}$ <br><br>
	 				<i>
	 					<small>
	 						Note that this is a tropical sum of tropical monomials
	 					</small>
	 				</i>
	 			</section>

	 			<section>
	 				<h3> Examples </h3>
	 				<p class="fragment">$4x^{10} \oplus 3x=$
	 					<span class="fragment">
	 						$\max(4 + 10x, 3 + x)$
	 					</span>
	 				</p>
	 				<br><br>
	 				<img src="2d1.png", class="fragment", style="border:0">
	 				<img src="2d2.png", class="fragment", style="border:0">
	 			</section>

	 			<section>
					<h3> Examples CONTD. </h3>
	 				<p>$x^3y^5\oplus 4x \oplus y^7=$
	 					<span class="fragment">
	 						$\max(1 + 3x + 5y, 4 + x, 1+7y)$
	 					</span>
	 					<br><br>
	 					<img src="3d1.png", class="fragment", style="border:0">
	 					<img src="3d2.png", class="fragment", style="border:0">
	 				</p>
	 			</section>

	 			<section>
	 				<h3> Tropical Rational Functions </h3>
	 				A tropical rational function is a tropical quotient of two tropical
	 				polynomials $f(x)$ and $g(x)$ <br><br>
	 			 	<div class="fragment">
	 			 		<h3> Example </h3>
	 			 		$(x_1^34x_2^5\oplus x_1)\oslash (x_2^5\oplus 2x_1x_2)$<br>
	 			 		<p class="fragment">
	 			 			$=\max(3x_1 + 4 + 5x_2, 1+x_1) - \max(1+5x_2, 2+x_1+x_2)$
	 					</p>
	 				</div>
	 		 </section>

	 		 <section>
	 			 <h3> The "magic" behind tropical rational functions </h3>
	 			 <table>
	 				 <tr>
	 					 <th style="font-size:24px;vertical-align: top">
	 						We will eventually be able to show equivalence between tropical
	 						rational functions and feedforward neural networks with ReLU activation.
	 			 	 	 </th>
	 			   <th>
	 			    <img src="nn_tr.png", style="border:0">
	 		     </th>
	 	      </tr>
	 	     </table>
	 		</section>

			<section>
				<h3> Tropical geometry </h3>
				Associated with this tropical algebra are many tropical analogues of
				notions from classical algebraic geometry
			</section>


			<section>
				<h3> Why Learn Tropical Hypersurfaces </h3>
				<p style="font-size:20px">
					In the second part of the series, we show that the decision boundary of a feed forward neural network with ReLU activaton
					is contained in the tropical hypersurface of a particular tropical polynomial.
        </p>
				<p style="font-size:20px">
					Tropical hypersurface divides the domain into convex cells where a polynomial is linear. The number of these linear cells
					gives us a measure of complexity of a neural network. Higher the number of linear cell, greater the ability of NN to
					discriminate date points into different classes.
				</p>
			</section>


			<section>
				<h3> Tropical hypersurfaces </h3>
				<p style="font-size:20px">
				The tropical hypersurface of a tropical polynomial
				$f(x) = c_1x^{\alpha_1}\oplus\dots\oplus c_rx^{\alpha_r}$ is the
				set of points $x$ at which the value of $f$ at $x$ is attained by two
				or more monomials in $f$.  <br><br>
		  	</p>
				$$\mathcal{T}(f) := \{x\in\mathbb{R}^d: c_ix^{\alpha_i}=c_jx^{\alpha_j}=f(x) \;
		    \mathrm{for\; some}\; \alpha_i \neq \alpha_j \}$$
			</section>
			<section>
				<h3> Examples </h3>
				    $$4x^{10} \oplus 3x= \max(4+10x, 3+x)$$ <br>
						<table>
							<tr>
								<th> Function plot </th>
								<th> Tropical hypersurface </th>
							</tr>
							<tr>
								<td> <img src="2d2_annotate.png", style="border:0"> </td>
								<td> <img src="2dhyper.png", style="border:0"> </td>
							</tr>
						</table>
      </section>
			<section>
				<h3> Examples CONTD. </h3>
				    $$1\odot x^2 \oplus 1\odot y^2 \oplus 2\odot xy \oplus 2 \odot x \oplus 2 \odot y\oplus 2$$
						<br>
						$$=\max(1+2x, 1+2y, 2+x+y, 2 + x, 2+y, 2)$$
						<br>
						<table>
							<tr>
								<th> Function plot </th>
								<th> Tropical hypersurface </th>
							</tr>
							<tr>
								<td> <img src="function3d.png", style="border:0", width="350px", height="250px"> </td>
								<td> <img src="trop_hyper3.png", style="border:0", width="350px", height="250px"> </td>
							</tr>
						</table>
      </section>


		<section>
		<h3> Why Learn Newton Polygons </h3>

			<p style="font-size:30px">The number of vertices in a Newton polygon gives us a lower bound on linear regions of a tropical polynomial.
			</p>
			<p style="font-size:30px">
			Each edge from the Newton polygon $\Delta(f)$ corresponds to an unbounded edge in $\mathcal{T}(f)$.
			</p>
		</section>

		<section>
			<h3> Convex Hull </h3>
			The convex hull of a set of points in $\mathbb{R}^d$ is the smallest convex
			set that contains all of the points.
			<br><br>
			<img src='convex_hull.png', style="border:0">
		</section>

		<section>
			<h3> Newton Polygons </h3>
			 <p style="font-size:30px">
				The Newton polygon of a tropical polynomial
				$f(x) = c_1x^{\alpha_1} \oplus\dots\oplus c_rx^{\alpha_r}$ is the convex hull
				of $\alpha_1, \dots, \alpha_r \in\mathbb{N}^d$ regarded as points in
				$\mathbb{R}^d$ <br> <br>
		  	</p>
				$$
				\triangle(f) := \mathrm{Conv}\{\alpha_i \in \mathbb{R}^d \; : \; c_i\neq -\infty,
				i=1,\dots,r\}
				$$ <br><br>
				<p style="font-size:30px">
				<i>
					Note: this is the convex hull of the powers of $x$.
				</i>
			</p>
		</section>

		<section>
			<h3> Examples </h3>
			$f(x)=4x^{10} \oplus 3x$
			<br>
			<p style="font-size:28px">
				Here we have $\alpha_1 = 10$ and $\alpha_2 =1$ <br><br>
			</p>
			<figure class="fragment">
				<figcaption style="font-size:20px">$\triangle(4x^{10}\oplus 3x)$</figcaption>
				<img src="newton2d.png", style="border:0">
			</figure>
			</section>

			<section>
				<h3> Examples CONTD. </h3>
				<p style="font-size:28px">
					$$f(x)=1\odot x_1^2 \oplus 1\odot x_2^2 \oplus 2\odot x_1x_2 \oplus 2 \odot x_1 \oplus 2 \odot x_2\oplus 2$$
				<br> $\alpha_1 = (2, 0), \alpha_2 = (0, 2), \alpha_3=(1, 1),\\ \alpha_4 = (1, 0), \alpha_5=(0, 1), \alpha_6 = (0, 0)$
			  </p>
				<br>
				<figure class="fragment">
					<figcaption style="font-size:20px"> $\triangle(f(x))$ </figcaption>
					<img src="np_3d.png", style="border:0">
				</figure>
	    </section>

			<section>
				<h3> Linear regions </h3>
				<p style="font-size:20px">
					A <b> linear region </b> of a tropical rational map $F$ is a maximal connected
					subset of the domain on which $F$ is linear.  <br><br>
					It turns out we will be able to use tropical geometry to bound the number
					of linear regions of the output of a neural network.
				</p>
				<img src="trop_hyper3_color.png", style="border:0">
			</section>

			<section>
 			 <h3> Why Learn Dual Subdivision? </h3>
 			 <p style="font-size:20px">
			 Lets denote Newton Polygon by $\Delta(f)$, $\\$
			 Tropical Hypersurface by $\mathcal{T}(f)$, $\\$
			 and dualsubdivision by $\delta(f)$
			 </p>
			 <p style="font-size:20px">
					 Each edge from the Newton polygon $\Delta(f)$ corresponds to an unbounded edge in $\mathcal{T}(f)$.
					 Each one-dimensional edge of a face in $\delta(f)$ corresponds to an edge in $\mathcal{T}(f)$.
			 </p>

			 <p style="font-size:20px">
			 So, dual subdivision gives us more information about the tropical hypersurface than the newton polygon.
 			 </p>
 		  </section>

			<section>
				<h3> Convex Polytope </h3>
				<p style="font-size:20px">
				A Convex polytope is a convex hull created by lifting each $\alpha_i$ from $\mathbb{R}^d$ into $\mathbb{R}^{d+1}$
				by appending $c_i$ as the last coordinate .
			  <br></br>
				Denote the convex hull of the lifted $\alpha_1$, $\dots$ , $\alpha_r$ as
			  </p>
			  <p style="font-size:30px">
				$\mathcal{P}(f)$ := Conv {($\alpha_i$,$c_i$) $\in$ $\mathbb{R}^{d+1}$ $\times$ $\mathbb{R}$ : i = 1, $\dots$, r }}
		    </p>

				<p style="font-size:20px">
				The convex hull may be defined either as the intersection of all convex sets containing X,
				or as the set of all convex combinations of points in X.
			  </p>
			</section>

			<section>
				<h3> Convex Polytope Example </h3>
				<p style="font-size:20px">
				Let's look at the convex polytope for the following example.
				</p>
				<p style="font-size:30px">
				$f(x_1,x_2) = 1 \odot x_1^2 \oplus 1 \odot x_2^2 \oplus 2 \odot x_1 x_2 \oplus 2 \odot x_1 \oplus 2 \odot x_2 \oplus 2$
				</p>
				<p style="font-size:20px">
				 $
         1 \odot x_1^2 \rightarrow (2,0,1), \quad   1 \odot x_2^2 \rightarrow (0,2,1), \\
				 2 \odot x_1 x_2 \rightarrow (1,1,2), \quad 2 \odot x_1 \rightarrow (1,0,2), \\
				 2 \odot x_2 \rightarrow (0,1,2),  \quad 2 \rightarrow (0,0,2)
				 $
				</p>
				<img src="Polytope.png", {:height="50%" width="50%"}, style="border:0">
				<img src="Legend.png",  {:height="10%" width="10%"}, style="border:0">
			</section>

			<section>
				<h3> Dual Subdivision </h3>
				<p style="font-size:20px">
				A tropical polynomial $f(x)$ determines a dual Subdivision of $\Delta (f)$, constructed
				as follows.
				<br> <br>
				Let $UF(\mathcal{P}(f))$ denote the collection of upper faces in
				$\mathcal{P}(f)$ and $\pi: \mathbb{R}^d \times \mathbb{R}$ be the projection
				that drops the last coordinate.
				<br></br>
				The dual subdivision determined by f is then
				<br></br>
				$\delta(f) := \{ \pi(p) \subset \mathbb{R}^d : p \in UF(\mathcal{P}(f)))\}$
		  	</p>
				<p style="font-size:20px">
				It turns out that each vertex in $\delta(f)$ corresponds to one "cell" in
				$\mathbb{R}^d$ where the function is linear. Thus, the number of vertices in
				$\mathcal{P}(f)$ provides and upper bound on the number of linear regions of f.
		  	</p>
			</section>

			<section>
			  <h3> Dual Subdivision Example </h3>

				<p style="font-size:30px">
				The below figure shows the dual Subdivision for the tropical polynomial
        </p>
        <p style="font-size:30px">
				$f(x_1,x_2)$ = $ 1 \odot x_1^2 \oplus 1 \odot x_2^2 \oplus 2 \odot x_1x_2 \oplus 2 \odot x_1 \oplus 2 \odot x_2 \oplus 2$
		  	</p>
				<img src="dualsubdivision.png", {:height="50%" width="50%"}, style="border:0">
			</section>


			<section>
				<h3> Transformation of Tropical Polytopes </h3>

				<p style="font-size:30px">
				Analysis of neural networks will require figuring out how the polytope
				$\mathcal{P}(f)$ transforms under tropical power, sum and product.
				</p>

				<p style="font-size:30px">
				Let $f$ be a tropical polynomial and let $a \in \mathbb{N}$. Then

			  <br> </br>
				$\mathcal{P}(f^a) = a\mathcal{P}(f)$.

				<br> </br>

				$a\mathcal{P}(f) = \{ ax : x \in \mathcal{P}(f) \} \subseteq \mathbb{R}^{d+1}$

				</section>

				<section>
					<h3> Transformation of Tropical Polytopes Contd.. </h3>
					<p style="font-size:30px">
					To describe the effect of tropical sum and product,
      		we define Minkowski Sum of two sets $P_1$ and $P_2$ in $\mathbb{R}^{d}$ is the set
          <br> </br>
					$P_1 + P_2 := \{ x_1 + x_2 \in \mathbb{R}^{d} : x_1 \in P_1, x_2 \in P_2 \}$
          <br> </br>
					In particular, the Minkowski sum of line segments is called a $zonotope$
          <br> </br>
					Let $\mathcal{V}(P)$ denote the set of vertices of polytope $P$.
					The Minkowski sum of two polytopes is given by the convex hull of the Minkowski
					sum of their vertex sets, i.e.,
					<br> </br>

					$P_1 + P_2 = Conv(\mathcal{V}(P_1) +\mathcal{V}(P_2))$.
					</p>
				</section>

				<section>
					<h3> Minkowski Sum Example </h3>
					<p style="font-size:20px">
					Let A be the vertex set containing $\{ (1,0) , (0,1) , (0,-1) \}$
					<br></br>
					Let B be the vertex set containing $\{ (0,0) , (1,1) , (1,-1) \}$
					<br></br>
					Then $A + B$
					</p>

					<img src="MinkowskiSum.png", {:height="70%" width="70%"}, style="border:0">

				</section>

				<section>
					<h3> Transformation of polytopes Contd.</h3>
					<p style="font-size:30px">
						Let $f$, $g$ $\in$ $Pol(d,1) = \mathbb{T}[x_1,\dots,x_d]$ be
						tropical polynomials. Then
            <br></br>
						$\mathcal{P}(f \odot g) = 	\mathcal{P}(f) + \mathcal{P}(g)$,
						<br></br>
						$\mathcal{P}(f \oplus g) = 	Conv(\mathcal{V}(\mathcal{P}(f)) \cup \mathcal{V}(\mathcal{P}(g)))$
					</p>
			  </section>

				<section>
					<h3> Transformation of polytopes Example </h3>
					<p style="font-size:20px">
					Let f be a tropical polynomial - $ 0 \odot x \oplus 1 \oplus -1 $.
					The coordinates of this tropical polynomial will be $\{ (1,0) , (0,1) , (0,-1) \}$
					<br></br>
					Let g be a tropical polynomial - $ 0 \oplus 1 \odot x \oplus -1 \odot x $.
					The coordinates of this tropical polynomial will be $\{ (0,0) , (1,1) , (1,-1) \}$
					<br></br>
					Then $\mathcal{P}(f \odot g) = 	\mathcal{P}(f) + \mathcal{P}(g)$,
					<br></br>
					$\mathcal{P}(f \oplus g) = 	Conv(\mathcal{V}(\mathcal{P}(f)) \cup \mathcal{V}(\mathcal{P}(g)))$
					</p>
					<img src="MinkowskiSum.png", {:height="50%" width="50%"}, style="border:0">

				</section>

				<section>
					<h3> BREAK TIME </h3>
           <p style="font-size:30px">
						 LETS TAKE A BREAK FOR 5 Mins
					 </p>
      		After Break, We'll put all of this together and talk about:
					<br><br>
					<ul style="font-size:32px">
						<li>
							<b>Tropical algebra of neural netowrks</b>
							<ul style="font-size:28px">
								<li>
									Recurrence relation for tropical rational maps of each layer
								</li>
								<li> Equivalence of tropical rational functions,
									continuous piecewise linear functions with integer coefficients,
									and ReLU neural networks satisfying certain conditions
								</li>
							</ul>
						</li>
						<br>
						<li>
							<b>Tropical geometry of neural networks</b>
							<ul style="font-size:28px">
								<li>
									Tropical geometry of decision boundary
								</li>
								<li>
									Geometric building blocks of neural networks
                </li>
								<li>
									Geometry complexity of deep neural networks
								</li>
							</ul>
						</li>
					</ul>
				</section>



				<section>
					<h3> Neural network recap </h3>
					<table style="font-size:24px">
						<tr>
							<td style="text-align:center"> NN layers <br>
								<img src="nn.png", {:height="200px" width="300px"}, style="border:0">
							</td>
							<td style="text-align:center"> Neuron <br>
								<img src="neuron.png", {:height="200px" width="300px"}, style="border:0">
							</td>
						</tr>
						<tr>
							<td style="text-align:center"> Decision Boundary <br>
								<img src="3LayerNNDB.png", {:height="200px" width="300px"}, style="border:0">
							</td>
							<td style="text-align:center"> Activation Function (ReLU) <br>
								<img src="relu.png", {:height="200px" width="300px"}, style="border:0">
							</td>
						</tr>
					</table>
				</section>
				<section>
					<h3>
						Tropical algebra recap
					</h3>
					<table style="width:20%">
	   				<tr>
	     				<th>Operation</th>
	     				<th>Notation</th>
	     				<th>Definition</th>
	   				</tr>
	 					<tr>
	 						<td>Addition</td>
	 						<td>$x\oplus y$</td>
	 						<td>$\max \{x, y\}$</td>
	 					</tr>
	 					<tr>
	 						<td>Multiplication</td>
	 						<td>$x\odot y$</td>
	 						<td>$x+y$</td>
	 					</tr>
	 					<tr>
	 						<td>Division</td>
	 						<td>$x\oslash y$</td>
	 						<td>$x-y$</td>
	 					</tr>
	 					<tr>
	 						<td>
	 							Power
	 							<small style="vertical-align:bottom">
	 								($a\in\mathbb{N}$)
	 							</small>
	 						</td>
	 						<td>$x^{\odot a}$</td>
	 						<td>$ax$</td>
	 				</table>
				</section>
				<section>
					<h3>
					More tropical algebra
				  </h3>
					<br>
					<table>
						<tr>
							<td style="vertical-align:middle">
								<b> Monomials </b>
							</td>
							<td>
							  $c\odot x^{\alpha} = c+\alpha x$
								<br>
						  </td>
						</tr>
						<tr>
							<td style="vertical-align:middle">
								<b> Polynomials </b>
							</td>
							<td>
							<br>
							$c_1x^{\alpha_1}\oplus \dots \oplus c_rx^{\alpha_r}$
		 					$$=\max(c_1 + \alpha_1x, \dots, c_r + \alpha_rx)$$
							<br>
							</td>
						</tr>
						<tr>
							<td style="vertical-align:middle">
								<br>
								<b> Rational Functions </b>
							</td>
							<td>
								<br>
								$f(x)\oslash g(x) = f(x) - g(x)$
								<br>
							</td>
						</tr>
					</table>
				</section>
				<section>
					<h2> Tropical algebra of neural networks </h2>
					<br>
					<b style="font-size:36px"> Assumptions </b>
					<br>
					<ul style="font-size:32px">
						<i>
							<li>
								Weight matrices of networks are integer valued
							</li>
							<li>
								Bias vectors are real-valued
							</li>
							<li>
								The activation function(s) take the form
								$\sigma(x) := \max\{x, t\}$
								where $t\in (\mathbb{R}\cup\{-\infty\})$
							</li>
						</i>
					</ul>
				</section>
				<section>
					<h3> Example network </h3>
					<img src="nn_ex2.png", style="border:0;width:600px;height:500px">
				</section>

				<section>
					<b> Finding a tropical rational map for a one layer network </b>
					<div class="container", style="display:flex;font-size:28px">
						<div class="col", style="flex:1">
							<br>
							Let's zoom in on the first layer of this network
							<img src="nn_zoom_l1.png", style="border:0;width:400px;height:400px">
						</div>
						<div class="col", style="flex:1">
							<br><br><br>
							<b style="text-align:center;vertical-align:top"> Layer output </b>
							<br>
							$$ \max\{A_1x + b_1, 0\}$$
							<br>
							<img src="matrix_ex_nn.png", style="border:0;width:400px;height:250px">
						</div>
					</div>
				</section>
				<section>
					<b> Where to start to decompose the network into a tropical rational function? </b>
					<div class="container", style="display:flex;font-size:28px">
						<div class="col", style="flex:1">
							<br>
							<b> We know </b>
							<br><br>
								<ul style="font-size:24px">
									<li> Tropical rational functions are differences of tropical polynomials </li><br>
									<li> Tropical polynomials are convex </li><br>
									<li> We have no reason to believe that the network output is convex </li>
								</ul>
							</div>
							<div class="col", style="flex:1">
								<p class="fragment", style="font-size:28px">
									How to decompose a layer of a network into its convex pieces?
								</p>
								<img src="nonconvex.png", style="border:0", class="fragment">
								<br>
								<p class="fragment", style="font-size:20px">
								Since our layer is a piecewise linear function, we can ensure each piece is convex by
								separating $A$ into its positive and negative components (which correspond to the
								slopes)
							</p>
							</div>
						</div>
					</section>

				<section class="center", style="margin-left:0">
					<h4><b> Example of a tropical map for a one layer network </b></h4>
					<br>
					<table class="fixed", style="width:400px">
						<col width="100px" />
						<col width="100px" />
						<col width="50px" />
					<tr>
						<td style="vertical-align:top;border:none">
							<p style="font-size:20px;text-align:center">
								We begin by decomposing $A_1$ into positive and negative components $A_{1+}$ and $A_{1-}$
								<br><br>
								$$
								A_1=
								\begin{bmatrix}
								-1&1\\
								1&-3\\
								1&2\\
								-4&1\\
								3&2
								\end{bmatrix}
								=
								\begin{bmatrix}
								0&1\\
								1&0\\
								1&2\\
								0&1\\
								3&2
								\end{bmatrix}
								-
								\begin{bmatrix}
								1&0\\
								0&3\\
								0&0\\
								4&0\\
								0&0
								\end{bmatrix}
								$$
							</p>
						</td>
						<td style="font-size:20px;text-align:center;vertical-align:center;border:none", class="fragment">
							<br>
							Note that the output of the layer $\max\{A_1x+b, 0\}$ can be written as
							$$\max\{A_{1+}x + b_1, A_{1-}x\} - \max\{A_{1-}x, -\infty\}$$
							<br>
							$$\max\left\{\begin{bmatrix}
							0&1\\
							1&0\\
							1&2\\
							0&1\\
							3&2
							\end{bmatrix}
							\begin{bmatrix}
							x_1\\
							x_2
							\end{bmatrix}
							+
							\begin{bmatrix}
							1\\
							-1\\
							2\\
							0\\
							-2
							\end{bmatrix},
							\begin{bmatrix}
							1&0\\
							0&3\\
							0&0\\
							4&0\\
							0&0
							\end{bmatrix}
							\begin{bmatrix}
							x_1 \\
							x_2
							\end{bmatrix}
							\right\}
							-\begin{bmatrix}
							1&0\\
							0&3\\
							0&0\\
							4&0\\
							0&0
							\end{bmatrix}
							\begin{bmatrix}
							x_1 \\
							x_2
							\end{bmatrix}
							$$
						</td>
					</tr>
					<tr>
						<td style="font-size:20px;text-align:right;border:none", class="fragment">
							Note that this is a tropical rational map.  This expression is equal to
						</td>
							<td class="fragment", style="font-size:20px;text-align:left">
								$$
								\begin{bmatrix}
								(x_2\oplus x_1)\oslash(x_1)\\
								(-x_1\oplus x_2^3)\oslash(x_2^3)\\
								(2x_1x_2^2)\oplus 0)\oslash 0\\
								(0\oplus x_1^4)\oslash(x_1^4) \\
								(-2x_1^3x_2^2\oplus 0)\oslash 0
								\end{bmatrix}
								$$
						</td>
					</tr>
				</table>
			</section>

				<section>
					<h4><b>
						Decomposition of multi-layer networks into tropical rational
						functions
					</b></h4>
					<table style="width:1100px;height:700px;table-layout:fixed;cell-padding:0px;rules:none">
						<tr style="font-size:18px">
							<td>
								Suppose the nodes of the $\ell$th layer of a multi-layer network
								are given by tropical rational functions:
								<br><br>
								$$ \nu^{\ell}(x)=F^{(\ell)}(x)\oslash G^{(\ell)}(x) = F^{(\ell)}(x) - G^{(\ell)}(x)$$
								<br>
								Then the outputs of the preactivation and of the $(\ell+1)$th layer are given
								by the tropical rational functions
								<br><br>
								$$
								\rho\circ\nu^{(\ell)}(x) = H^{(\ell+1)}(x) - G^{(\ell+1)}(x)
								$$
								$$
								\nu^{(\ell+1)}(x) = \sigma_{(\ell + 1)}\circ\rho\circ \nu^{(\ell)}(x)
								=F^{(\ell+1)}(x) - G^{(\ell+1)}(x)
								$$
								<br>
								respectively, where
								<br><br>
								$$
								F^{(\ell+1)}(x) = \max\{H^{(\ell+1)}(x) - G^{(\ell+1)}(x)\} \;\;\;\;\;\;\;\;
								f_{i}^{l+1} = h_{i}^{l+1} \oplus (g_i^{l+1} \odot t_{i})
								$$
								$$
								G^{(\ell+1)}(x) = A_{-}^{(\ell+1)}+G^{(\ell)}(x) + A_{-}^{(\ell+1)}{-}F^{(\ell)}(x) \; \;\;\;\;\;\;\;
								g_{i}^{l+1} = [ \bigodot_{j=1}^{n} (f_{j}^{l})^ {a_{ij}^{-}} ] \odot [ \bigodot_{j=1}^{n} (g_{j}^{l})^{a_{ij}^{+}} ]
								$$
								$$
								H^{(\ell+1)}(x) = A_+^{(\ell+1)}F^{(\ell)}(x) + A_{-}G^{(\ell)}(x) + b^{(\ell+1)}\;\;\;\;
								h_{i}^{l+1} = [\bigodot_{j=1}^{n} (f_{j}^{l})^{a_{ij}^{+}}] \odot [\bigodot_{j=1}^{n} (g_{j}^{l})^{a_{ij}^{-}}] \odot b_{i}
								$$
								<p style='font-size:20px'>
									<b> Bottom line: <i>If a one layer network
										can be written as a tropical rational map,
										then a network with <font style="color:purple">
											multiple layers</font> can be written as a
											<font style="color:purple"> tropical rational map</font></i>
									</b>
									<br>
									<br> <b><i> A ReLU activated
										feedforward <font style="color:purple">neural network </font> with integer weights and
										linear output is a <font style="color:purple"> tropical rational function </font></b></i>
								</p>
							</td>
						</tr>
					</table>
				</section>

			<section>
 				<h3><b> Equivalence of Neural Networks and Tropical Rational functions </b></h3>
 				<ul style="font-size:28px">
	 				 <li>
	 					 Let $\mathcal{v}: \mathbb{R}^{d+1} \rightarrow \mathbb{R}$.
						 Then $\mathcal{v}$ is a tropical rational function if and only if $\mathcal{v}$ is a
						 feedforward neural network with ReLU activation.
	 				 </li><br>
	 				 <li>
						 A tropical rational function $f \oslash g$ can be represented as an L-layer neural network, with
						 $ L \leq max\{\lceil log_2 r_f \rceil , \lceil log_2 r_g \rceil \} + 2 $,
						 where $r_f$ and $r_g$ are the number of monomials in the tropical polynomials $f$ and $g$ respectively.
	 				 </li>
	 			</ul>
     	</section>

			<section>
				<h3><b> More about Tropical Rational functions</b> </h3>
				<ul style="font-size:30px">
					 <li>
						 Tropical rational function $f \oslash g : \mathbb{R}^{d+1} \rightarrow \mathbb{R}$ is a continuous
						 piecewise linear function.
					 </li><br>
					 <li>
						 Tropical hyper surface of a tropical rational function divide $\mathbb{R}^{d}$ into polyhedral regions
						 on each of the which $f \oslash g$ is linear, although these regions are $nonconvex$ in general.
					 </li>
				</ul>
			</section>

			<section>
				<h4><b> Let's return to our example network</b></h4>
				<img src="nn_ex2.png", style="border:0;width:600px;height:500px">
			</section>
			<section>
				<b> After some computation, network output is... </b>
				<br>
				<p style="font-size:24px">
				\begin{multline}
				\max\{(x_2\oplus x_1^4)\odot((-2)\odot x_1^3x_2^2\oplus0)^3\odot x_1\odot(x_2^3)^2,\\
					(1\odot x_2\oplus x_1)\odot((-1)\odot x_1\oplus x_2^3)^2\odot(2\odot x_1x_2^2\oplus 0)\odot x_1^4\}\\
					\oslash (x_2\oplus x_1^4)\odot((-2)\odot x_1^3x_2^2\oplus 0)^3\odot x_1\odot(x_2^3)^2
				\end{multline}
				</p>
				<img src="ex_network.png", style="border:0;width:300px;height:300px">
			</section>

			<section>
				<b><h4> Tropical hypersurface and dual subdivision for the example network output</b></h4>
				<p style="font-size:24px">
					Assuming $\nu(x)=f(x)\oslash g(x)$, we have the following polytopes and dual subdivisions
					for the example network.
				</p>
				<div class="container", style="display:flex;font-size:28px">
					<div class="col", style="flex:1">
						<p style="text-align:center">
							<b>$f(x)$</b>
						</p>
						<img src="dual_f.png", style="border:0;width:300px;height:300px", title="stuff">
					</div>
					<div class="col", style="flex:1">
						<p style="text-align_center">
							<b> $g(x)$ </b>
						</p>
						<img src="dual_g.png", style="border:0;width:300px;height;300px">
					</div>
				</div>
			</section>
			<section>
 			 <h3><b>Equivalence of Continuous linear function and Tropical Rational functions</b> </h3>
 			 <p style="font-size:30px">
				 Let $\mathcal{v}: \mathbb{R}^{d+1} \rightarrow \mathbb{R}^{d}$. Then $\mathcal{v}$ is a continuous piecewise
				 linear function with integer coefficients if and only if $\mathcal{v}$ is a tropical rational function.
 			 </p>
    	 <p style="font-size:50px">
			 <b>BRINGING EVERYTHING TOGETHER</b>
			 <ul style="font-size:30px">
					<li>
						Tropical rational functions.
					</li>
					<li>
						Continuous piecewise Linear functions with integer coefficients.
					</li>
					<li>
						feedforward NN with ReLU activation.
					</li>
			 </ul>
 		 </section>

		 <section>
			<h3><b>Decision boundaries of neural network</b> </h3>
      <p style="font-size:30px">
			 Lets study the decision boundaries of neural networks, focusing on the case of two-category classification.
			</p>

			<p style="font-size:30px">
				$\mathcal{v}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{p}$ together with a choice of score function
				$s: \mathbb{R}^{p} \rightarrow \mathbb{R}$ gives us such a classifier.
			</p>

			<p style="font-size:30px">
			If the output value $s(v(x))$ exceeds some decision threshold $c$, then the neural network predicts x is from class,
			and otherwise x is from the other category.
		 </section>

		 <section>
			<h3><b>Decision boundaries of neural network contd.</b> </h3>
			 <p style="font-size:30px">
			 The input space is thereby partitioned into two disjoint subsets by the decision boundary.
			</p>

			<p style="font-size:30px">
      $\mathcal{B} := \{x \in \mathbb{R}^d: \mathcal{v}(x) = s^{-1} (c)\}$.
			</p>

			<p style="font-size:30px">
			Connected regions with value above the threshold and connected regions with value below the threshold will be called the positive
			and negative regions respectively.
		 </section>

		 <section>
			<h3><b>Tropical geometry of decision Boundary</b> </h3>
			 <p style="font-size:30px">
			 The following two results give us the bounds on the number of positive and negative regions and that there is a
			 tropical polynomial whose tropical hypersurface contains the decision boundary.
			<br></br>
			<b> SOME NOTATION </b>
			<br></br>
			Let $\mathcal{v}:\mathbb{R}^{d} \rightarrow \mathbb{R}$ be an L-layer feedforward neural network
			with ReLU activation and $t^{L}= -\infty$. Let the score fuunction $s: \mathbb{R} \rightarrow \mathbb{R}$ be injective
			with decision threshold c in its range. If $\mathcal{v} = f \oslash g$ where f and g are tropical polynomials, then
			</p>
     </section>

		 <section>
			<h3><b>Tropical geometry of decision Boundary contd.</b> </h3>
			<ul style="font-size:30px">
				 <li>
					 Its decision boundary $\mathcal{B} := \{x \in \mathbb{R}^d: \mathcal{v}(x) = s^{-1} (c)\}$.
					 divides $\mathbb{R}^{d+1}$ into at most $\mathcal{N}(f)$ connected positive regions and at most
					 $\mathcal{N}(g)$ connected negative regions.
				 </li>
			</ul>
			<p style="font-size:30px">
			Where  $\mathcal{N}(f)$ and  $\mathcal{N}(g)$  are the number of linear regions in tropical polynomial $f$ and $g$.
		  </p>
		 </section>

		 <section>
			<h3><b>Tropical geometry of decision Boundary contd.</b> </h3>
			<ul style="font-size:30px">
				 <li>
					 Its decision boundary $\mathcal{B} := \{x \in \mathbb{R}^d: \mathcal{v}(x) = s^{-1} (c)\}$ is
					 contained in the tropical hypersurface of the tropical polynomial $ s^{-1} (c) \odot g(x) \oplus f(x)$
					 i.e., $ max \{  f(x), s^{-1} (c) + g(x) \}$
				 </li>
			</ul>
				 <p style="font-size:30px">
				 $\mathcal{B} \subseteq  \mathcal{T} (s^{-1} (c) \odot g(x) \oplus f(x))$
			   </p>
			<ul style="font-size:30px">
				<li>
				The function $ s^{-1} (c) \odot g(x) \oplus f(x)$ is not necessarily linear on every positive or negative region
			  </li>
			</ul>
     </section>

		 <section>
		 <h3><b>Tropical geometry of decision Boundary contd.</b> </h3>
			 <p style="font-size:30px">
			 $\mathcal{B} \subseteq  \mathcal{T} (s^{-1} (c) \odot g(x) \oplus f(x))$
			 </p>
			<ul style="font-size:30px">
		    <li>
				The tropical hypersurface $ \mathcal{T} (s^{-1} (c) \odot g(x) \oplus f(x))$ may further divide a positive or negative region derived
				from $\mathcal{B}$ into multiple linear regions.
		  	</li>
				<li>
				 Hence the $ \subseteq $ cannot in general be replaced by $"="$
			  </li>
			</ul>
		  </p>
		 </section>

		 <section>
			 	<h3><b>Zonotopes - Building blocks of NN's</b> </h3>
				<p style="font-size:30px">
				How are the tropical hypersurfaces of the tropical polynomials in the $(l)^{th}$ layer of a neural network
				related to those in the $(l-1)^{th}$ layer?
			  </p>
        <p style="font-size:30px">
				Let $f_i^{l}, g_i^{l}, h_i^{l}$ be the tropical polynomials produced by the $i^{th}$ node in the $l^{th}$
				layer of a neural network.
				<ul style="font-size:30px">
					<li>
						$f_{i}^{l} = h_{i}^{l} \oplus (g_i^{l} \odot t_{i}) $
					</li>
					<li>
						$g_{i}^{l} = [ \bigodot_{j=1}^{n} (f_{j}^{l-1})^ {a_{ij}^{-}} ] \odot [ \bigodot_{j=1}^{n} (g_{j}^{l-1})^{a_{ij}^{+}} ] $
					</li>
					<li>
						$h_{i}^{l} = [\bigodot_{j=1}^{n} (f_{j}^{l-1})^{a_{ij}^{+}}] \odot [\bigodot_{j=1}^{n} (g_{j}^{l-1})^{a_{ij}^{-}}] \odot b_{i}$
					</li>
				</ul>
		 </section>

		 <section>
			 	<h3><b>Zonotopes - Building blocks of NN's Contd.</b> </h3>
				<p style="font-size:30px">
				  We also know that if $f$, $g$ $\in$ $Pol(d,1) = \mathbb{T}[x_1,\dots,x_d]$ be
					tropical polynomials. Then
				</p>
				<ul style="font-size:30px">
				<li>
				$\mathcal{P}(f^a) = a\mathcal{P}(f)$.
		  	</li>
				<li>
				$\mathcal{P}(f \odot g) = 	\mathcal{P}(f) + \mathcal{P}(g)$
		  	</li>
				<li>
				$\mathcal{P}(f \oplus g) = 	Conv(\mathcal{V}(\mathcal{P}(f)) \cup \mathcal{V}(\mathcal{P}(g)))$
			  </li>
			 </ul>
		 </section>

		 <section>
			 <h3><b>Zonotopes - Building blocks of NN's Contd.</b> </h3>
		   <p style="font-size:20px">
			 $\mathcal{P}(g_{i}^{1})$ and $\mathcal{P}(h_{i}^{1})$ are points.
			 $\mathcal{P}(f_{i}^{1})$ is a line segment.
		   </p>
			 <img src="linesegments.png", style="border:0;width:500px;height:300px">
		 </section>

		 <section>
		 <h3><b>Zonotopes - Building blocks of NN's Contd.</b> </h3>
	   <ul style="font-size:30px">
		 <li>
		  Continuing for Layer 2: $\mathcal{P}(g_{i}^{2})$ and $\mathcal{P}(h_{i}^{2})$ are zonotopes.
		 </li>
	   </ul>
		 <img src="zonotopes.png", style="border:0;width:500px;height:300px">
		 </section>

		 <section>
		 <h3><b>Zonotopes - Building blocks of NN's Contd.</b> </h3>
	   <p style="font-size:20px">
		  Continuing for Layer 2: $\mathcal{P}(f_{i}^{2})$.
		 </p>
		 <img src="secondlayer.png", style="border:0;width:600px;height:400px">
		 </section>

		 <section>
 		 <h3><b>Geometric Complexity of Deep NN's</b> </h3>
      <p style="font-size:30px">
		  The number of linear regions of a NN given by $\mathcal{v} : \mathcal{R}^{d} \rightarrow \mathcal{R}^{p}$
			is bounded from above by $\mathcal{O}(n^{d(L-1)})$
		  </p>
			<ul style="font-size:30px">
			 <li>
				 n is the number of Neurons
			 </li>
			 <li>
				 L is the number of layers
			 </li>
			 <li>
				 d is the dimension of the input data.
			 </li>
		  </ul>
		  <p style="font-size:30px;color:purple">
				Hence we surmise that the number of linear regions of the neural network grows
				polynomially with the n and exponentially with the number of Layers L.
		  </p>
	   </section>

		<section>
	 		<h3><b>What's Next in the field</b> </h3>
	 		<ul style="font-size:30px">
			 <li>
				 Stability analysis of NNs.
 	 		 </li>
    	 <li>
	 	     Effect of drop outs on NN decision boundary. Might give us a better way to do drop out instead of random drop outs
	 		 </li>
	 		 <li>
	 			 Extension of this theory to other activation functions.
	 		 </li>
	 		 <li>
	 			 Extesion of this theory to CNN's and RNN's
	 		 </li>
	 		</ul>
	 	</section>

    <section>
     <h3><b>Conclusion</b> </h3>
		 <ul style="font-size:30px">
			<li>
				Feedforward neural networks with rectified linear units are nothing more than tropical rational maps.
			</li>
			<li>
			  Complexity of a neural network grows polynomially with number of neurons, and exponentially with number of
				Layers L.
			</li>
			<li>
	      In other words, deep neural network is exponentially more expressive than shallow neural network.
			</li>
		 </ul>
		</section>

		 <section>
		 <h3><b>Thank You</b> </h3>
		 <p style="font-size:40px;color:purple">
	      Questions??
		 </p>
	   </section>

	   </div>
	   </div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				math: {
    			mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
    			config: 'TeX-AMS_HTML-full' // See https://docs.mathjax.org/en/latest/config-files.html
  			},
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
				  { src: 'plugin/math/math.js',async: true}
				]});

			Reveal.configure({ slideNumber: true });

		</script>
	</body>
</html>
